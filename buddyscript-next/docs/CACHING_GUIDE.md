# Redis Caching Implementation Guide

## Overview

This application implements a comprehensive Redis caching strategy using **Upstash Redis** and **ioredis** to handle millions of post reads efficiently. The caching layer uses a **Cache-Aside (Lazy Loading)** pattern with intelligent invalidation strategies.

## Architecture

### Technology Stack
- **Redis Provider**: Upstash Redis (Serverless Redis with global replication)
- **Client Library**: ioredis v5.x
- **Connection**: TLS-encrypted (rediss:// protocol)
- **Pattern**: Cache-Aside with automatic invalidation

### Key Components

1. **Redis Client** (`/src/lib/redis.ts`)
   - Singleton pattern for connection management
   - Automatic reconnection with exponential backoff
   - Connection pooling and keep-alive
   - Graceful error handling

2. **Cache Service** (`/src/services/cache.service.ts`)
   - Generic caching operations (get, set, delete)
   - Pattern-based cache invalidation
   - TTL management
   - Cache statistics and monitoring

3. **Service Layer Integration**
   - Post Service: Feed and individual post caching
   - Like Service: Like count caching with invalidation
   - Comment Service: Comment count caching with invalidation

## Cache Strategy

### What We Cache

#### 1. Post Feeds (5 minutes TTL)
```
Key: posts:feed:{userId}:{cursor}:{limit}
Value: Paginated list of posts with metadata
TTL: 300 seconds (5 minutes)
```

#### 2. Individual Posts (10 minutes TTL)
```
Key: post:{postId}
Value: Complete post data with author info
TTL: 600 seconds (10 minutes)
```

#### 3. Like Counts (2 minutes TTL)
```
Key: post:{postId}:likes:count
Value: Total like count
TTL: 120 seconds (2 minutes)
```

#### 4. Comment Counts (2 minutes TTL)
```
Key: post:{postId}:comments:count
Value: Total comment count
TTL: 120 seconds (2 minutes)
```

### Cache Invalidation Strategy

#### On Post Creation
- ‚úÖ Invalidate all feed caches (public and user-specific)
- ‚úÖ New post will appear on next cache miss

#### On Post Deletion
- ‚úÖ Delete specific post cache
- ‚úÖ Delete like/comment count caches for that post
- ‚úÖ Invalidate all feed caches

#### On Like/Unlike
- ‚úÖ Delete like count cache for the post
- ‚úÖ Delete post cache (to update hasUserLiked)
- ‚úÖ Feed caches remain valid (optimization)

#### On Comment Add/Delete
- ‚úÖ Delete comment count cache for the post
- ‚úÖ Delete post cache (to update comment count)
- ‚úÖ Feed caches remain valid (optimization)

## Configuration

### Environment Variables

Add to your `.env.local`:
```env
REDIS_URL="rediss://default:YOUR_PASSWORD@your-instance.upstash.io:6379"
```

### Connection Settings

The Redis client is configured with:
- **Max Retries**: 3 attempts per request
- **Retry Strategy**: Exponential backoff (max 2 seconds)
- **Connection Timeout**: 10 seconds
- **Keep-Alive**: 30 seconds
- **Offline Queue**: Enabled (buffers commands when disconnected)
- **TLS**: Automatically enabled via rediss:// protocol

## Usage Examples

### Caching in Services

```typescript
// Example: Get post with caching
async getPostById(postId: string): Promise<PostDTO> {
  const cacheKey = CacheKeys.post(postId);
  
  // Try cache first
  const cached = await cacheService.get<PostDTO>(cacheKey);
  if (cached) {
    return cached;
  }
  
  // Cache miss - fetch from database
  const post = await postRepository.findById(postId);
  
  // Cache the result
  await cacheService.set(cacheKey, post, 600); // 10 min TTL
  
  return post;
}
```

### Cache Invalidation

```typescript
// Example: Invalidate cache on mutation
async deletePost(postId: string): Promise<void> {
  await postRepository.delete(postId);
  
  // Invalidate caches
  await cacheService.del(CacheKeys.post(postId));
  await cacheService.delPattern(CacheKeys.patterns.allFeeds);
}
```

### Cache Key Builders

Use the provided key builders for consistency:

```typescript
import { CacheKeys } from '@/services/cache.service';

// Individual keys
CacheKeys.post(postId);
CacheKeys.postsFeed(userId, cursor, limit);
CacheKeys.postLikeCount(postId);
CacheKeys.postCommentCount(postId);

// Pattern matchers for bulk deletion
CacheKeys.patterns.allPosts;        // "post:*"
CacheKeys.patterns.allFeeds;        // "posts:feed:*"
CacheKeys.patterns.userFeeds(userId); // "posts:feed:{userId}:*"
CacheKeys.patterns.postData(postId); // "post:{postId}*"
```

## Performance Benefits

### Before Caching
- Every post read = 3-4 database queries (post + likes + comments + author)
- Feed load = N * 4 queries (N posts √ó 4 queries each)
- High database load
- Slow response times under load

### After Caching
- Cache Hit = 1 Redis query (< 1ms with Upstash global network)
- Cache Miss = 1 Redis query + database queries + 1 cache write
- ~90% reduction in database queries (typical cache hit rate)
- Sub-millisecond response times for cached data
- Horizontal scalability via Redis

### Expected Metrics
- **Cache Hit Rate**: 85-95% (for read-heavy workloads)
- **Response Time**: < 10ms for cache hits
- **Database Load**: Reduced by 80-90%
- **Throughput**: 10-50x improvement for popular posts

## Monitoring and Debugging

### Cache Statistics

Get real-time cache stats:

```typescript
import { cacheService } from '@/services/cache.service';

const stats = await cacheService.getStats();
console.log(stats);
// {
//   connected: true,
//   keyCount: 1523,
//   memoryUsage: "2.5MB"
// }
```

### Logging

Cache operations are automatically logged:
- ‚úÖ `Cache HIT` - Data found in cache
- ‚ùå `Cache MISS` - Data not in cache, fetching from DB
- üóëÔ∏è `Cache Invalidation` - Keys deleted from cache
- üîÑ `Redis reconnecting` - Connection recovery in progress
- ‚ùå `Redis error` - Connection or operation errors

### Debugging Tips

1. **Check Redis Connection**:
   ```typescript
   import { isRedisAvailable } from '@/lib/redis';
   console.log('Redis available:', isRedisAvailable());
   ```

2. **Monitor Cache Keys**:
   ```bash
   # Using redis-cli
   redis-cli -h your-host -p 6379 --tls -a YOUR_PASSWORD KEYS "posts:feed:*"
   ```

3. **Clear All Cache** (Development only):
   ```typescript
   await cacheService.flushAll();
   ```

## Fallback Strategy

The caching layer is designed to **gracefully degrade**:
- If Redis is unavailable, the app continues to work (using database only)
- Cache operations fail silently without throwing errors
- Logs warnings but doesn't crash the application

```typescript
// All cache operations check availability first
async get<T>(key: string): Promise<T | null> {
  if (!isRedisAvailable()) {
    return null; // Graceful fallback
  }
  // ... cache logic
}
```

## Best Practices

### DO ‚úÖ
- Use consistent cache key naming via `CacheKeys` builders
- Set appropriate TTL based on data volatility
- Invalidate caches on mutations
- Monitor cache hit rates in production
- Use pattern-based deletion for related keys

### DON'T ‚ùå
- Don't cache user-specific data in shared keys
- Don't set TTL too high (stale data risk)
- Don't forget to invalidate on updates
- Don't cache sensitive data without encryption
- Don't use `flushAll()` in production

## Upstash-Specific Features

### Global Replication
- Upstash provides automatic global replication
- Read requests served from nearest region
- < 10ms latency worldwide

### Pricing Model
- Pay-per-request pricing (no idle costs)
- First 10,000 requests/day free
- Ideal for serverless applications

### Monitoring
- Built-in metrics in Upstash dashboard
- Request count, cache hit rate, latency
- No additional APM tools needed

## Scaling Considerations

### Current Implementation
- Handles millions of reads per day
- Suitable for 10K-100K concurrent users
- ~1M cache operations per hour

### Future Optimizations
1. **Read Replicas**: Use Redis read replicas for read-heavy workloads
2. **Cache Warming**: Pre-populate cache for popular content
3. **Predictive Caching**: Cache next page based on user behavior
4. **Compression**: Compress large cached values
5. **Edge Caching**: Add CDN layer for static content

## Troubleshooting

### Issue: High Cache Miss Rate
**Symptoms**: Slow response times, high database load
**Solutions**:
- Increase TTL values
- Check if cache invalidation is too aggressive
- Monitor memory usage (eviction policy)

### Issue: Stale Data
**Symptoms**: Users see outdated information
**Solutions**:
- Reduce TTL values
- Ensure proper cache invalidation on mutations
- Add cache versioning

### Issue: Redis Connection Errors
**Symptoms**: `Redis error` in logs
**Solutions**:
- Check REDIS_URL in environment
- Verify Upstash instance is running
- Check network/firewall settings
- Verify TLS certificate validity

### Issue: Memory Usage Too High
**Symptoms**: Upstash memory limit reached
**Solutions**:
- Reduce TTL values
- Implement LRU eviction policy
- Compress cached values
- Review what's being cached

## Testing

### Unit Tests
```typescript
import { cacheService } from '@/services/cache.service';

describe('Cache Service', () => {
  it('should cache and retrieve data', async () => {
    await cacheService.set('test-key', { foo: 'bar' }, 60);
    const result = await cacheService.get('test-key');
    expect(result).toEqual({ foo: 'bar' });
  });
});
```

### Integration Tests
```typescript
describe('Post Service with Cache', () => {
  it('should return cached post on second call', async () => {
    const post1 = await postService.getPostById('123');
    const post2 = await postService.getPostById('123');
    // Second call should be faster (cache hit)
  });
});
```

## Maintenance

### Regular Tasks
- **Weekly**: Review cache hit rates in Upstash dashboard
- **Monthly**: Analyze memory usage and optimize TTL
- **Quarterly**: Review caching strategy and patterns

### Monitoring Alerts
Set up alerts for:
- Cache hit rate < 80%
- Redis connection errors > 10/hour
- Memory usage > 90%
- Request latency > 100ms

## Additional Resources

- [ioredis Documentation](https://github.com/redis/ioredis)
- [Upstash Redis Documentation](https://upstash.com/docs/redis)
- [Redis Caching Best Practices](https://redis.io/docs/manual/patterns/)
- [Cache-Aside Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)

---

**Last Updated**: November 24, 2025  
**Maintainer**: Development Team  
**Version**: 1.0.0
